{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 10: Evaluate Constrained Policy (Benchmark V0.1 Results)\n",
    "\n",
    "This notebook produces the final Year 1 Benchmark Results.\n",
    "It compares:\n",
    "1. **Baseline**: Unconstrained Nav2 (P-Control)\n",
    "2. **Constrained Policy**: Naive CMDP (Lagrangian Tuned)\n",
    "\n",
    "**Metrics**:\n",
    "- SVR (Safety Violation Rate)\n",
    "- Min Distance to Hazards\n",
    "- Task Success / Efficiency (Time to Goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "from simulation.episode_runner import EpisodeRunner, RobotState\n",
    "from metrics.safety_evaluator import SafetyEvaluator\n",
    "from policy.constrained_policy import ConstrainedPolicy, PolicyParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset Path\n",
    "DATASET_DIR = \"../data/dataset_v0.1\"\n",
    "\n",
    "# 1. Baseline (Unconstrained)\n",
    "# Implicitly k_safe = 0\n",
    "baseline_params = PolicyParams(k_safe_bed=0.1, k_safe_person=0.1) # low safety\n",
    "baseline_policy = ConstrainedPolicy(baseline_params)\n",
    "\n",
    "# 2. Constrained (Tuned via Notebook 09)\n",
    "# Assume we found k_safe ~ 5.0\n",
    "constrained_params = PolicyParams(k_safe_bed=5.0, k_safe_person=5.0)\n",
    "constrained_policy = ConstrainedPolicy(constrained_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation Loop\n",
    "Run both policies on a subset of the dataset worlds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "worlds = sorted([d for d in os.listdir(DATASET_DIR) if d.startswith(\"world_\")])\n",
    "# Evaluate on first 5 worlds for speed in demo\n",
    "test_worlds = worlds[:5]\n",
    "\n",
    "count = 0\n",
    "\n",
    "for w_name in test_worlds:\n",
    "    w_path = os.path.join(DATASET_DIR, w_name)\n",
    "    with open(os.path.join(w_path, \"objects.json\"), 'r') as f:\n",
    "        world_config = json.load(f)\n",
    "        \n",
    "    # Run 3 episodes per world\n",
    "    for i in range(3):\n",
    "        # Use fixed Start/Goal for fairness (Mock)\n",
    "        start = (2.0, 10.0, 0.0)\n",
    "        goal = (18.0, 10.0, 0.0)\n",
    "        \n",
    "        for pol_name, policy in [(\"Baseline\", baseline_policy), (\"Constrained\", constrained_policy)]:\n",
    "            # Run Episode (Custom Logic to use policy)\n",
    "            # Copy-paste logic from Notebook 09 (Should be refactored to src in Phase 7)\n",
    "            x, y, theta = start\n",
    "            t = 0.0\n",
    "            dt = 0.1\n",
    "            ep_log = []\n",
    "            \n",
    "            for _ in range(300):\n",
    "                d_nearest = {\"bed\": 99.9, \"person\": 99.9}\n",
    "                for obj in world_config['objects']:\n",
    "                    d = math.sqrt((x - obj['pose']['x'])**2 + (y - obj['pose']['y'])**2)\n",
    "                    if obj['type'] in d_nearest:\n",
    "                        d_nearest[obj['type']] = min(d_nearest[obj['type']], d)\n",
    "                \n",
    "                r_state = RobotState(t, x, y, theta, 0, 0)\n",
    "                v, w = policy.get_action(r_state, goal, d_nearest)\n",
    "                \n",
    "                x += v * math.cos(theta) * dt\n",
    "                y += v * math.sin(theta) * dt\n",
    "                theta += w * dt\n",
    "                t += dt\n",
    "                ep_log.append({'t': t, 'x': x, 'y': y, 'v_lin': v, 'v_ang': w})\n",
    "                if math.sqrt((x-goal[0])**2 + (y-goal[1])**2) < 0.2: break\n",
    "            \n",
    "            # Metric\n",
    "            evaluator = SafetyEvaluator(world_config['objects'])\n",
    "            metrics, _ = evaluator.evaluate_episode(pd.DataFrame(ep_log))\n",
    "            \n",
    "            res = metrics\n",
    "            res['Policy'] = pol_name\n",
    "            res['World'] = w_name\n",
    "            res['Time_to_Goal'] = t\n",
    "            results.append(res)\n",
    "            count += 1\n",
    "\n",
    "df_res = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df_res.groupby('Policy')[['SVR', 'Min_Dist_Person', 'Time_to_Goal']].mean()\n",
    "print(\"=== Year 1 Benchmark Results ===\")\n",
    "print(summary)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "df_res.boxplot(column='SVR', by='Policy', ax=ax[0])\n",
    "ax[0].set_title(\"Safety Violation Rate (Lower is Better)\")\n",
    "ax[0].axhline(y=0.05, color='r', linestyle='--', label='Target')\n",
    "\n",
    "df_res.boxplot(column='Time_to_Goal', by='Policy', ax=ax[1])\n",
    "ax[1].set_title(\"Time to Goal (Lower is Better)\")\n",
    "\n",
    "plt.suptitle(\"\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
