{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 13: Deploy VLA Policy (Inference)\n",
    "\n",
    "This notebook demonstrates **Closed-Loop Control** using the trained **Llama 3.1 VLA Policy**.\n",
    "It loads the LoRA adapter trained in Notebook 12 and runs it in the simulator.\n",
    "\n",
    "**Pipeline**:\n",
    "1. **Observation**: Robot State + Semantic Map (Text Description).\n",
    "2. **Reasoning**: VLA Model generates action text (e.g., \"Action: v_lin=0.3, v_ang=0.1\").\n",
    "3. **Execution**: Parse text -> Twist Command -> Simulator Step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Setup\n",
    "!pip install -q transformers accelerate bitsandbytes peft pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# Add src to path\n",
    "try:\n",
    "    sys.path.append(os.path.abspath('../src'))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from simulation.episode_runner import EpisodeRunner, RobotState\n",
    "from world_gen.hospital_generator import HospitalGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# ADAPTER_PATH = \"../models/vla_llama_v1\" # Result from Nb 12\n",
    "\n",
    "# Mock Loading for CI/Demo without GPU\n",
    "class MockLLM:\n",
    "    def generate(self, prompt):\n",
    "        return \"Action: v_lin=0.2, v_ang=0.0\"\n",
    "\n",
    "use_mock = not torch.cuda.is_available()\n",
    "\n",
    "if use_mock:\n",
    "    print(\"GPU not found. Using Mock LLM for logic verification.\")\n",
    "    model = MockLLM()\n",
    "else:\n",
    "    # Real Loading Logic\n",
    "    # bnb_config = BitsAndBytesConfig(load_in_4bit=True, ...)\n",
    "    # base_model = AutoModelForCausalLM.from_pretrained(MODEL_ID, ...)\n",
    "    # model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Policy Wrapper\n",
    "Converts Robot State to Prompt, queries LLM, parses Action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLAPolicy:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def get_action(self, state: RobotState, goal: tuple, nearest_dist: float):\n",
    "        # 1. Create Prompt (Must match training distribution)\n",
    "        prompt = f\"\"\"Control a robot in a hospital.\n",
    "State: x={state.x:.2f}, y={state.y:.2f}, theta={state.theta:.2f}.\n",
    "Goal: x={goal[0]:.2f}, y={goal[1]:.2f}.\n",
    "Sensors: Nearest obstacle at {nearest_dist:.2f}m.\n",
    "Output the linear and angular velocity safely.\"\"\"\n",
    "        \n",
    "        # 2. Query Model\n",
    "        if use_mock:\n",
    "            output_text = self.model.generate(prompt)\n",
    "        else:\n",
    "            # inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            # outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "            # output_text = tokenizer.decode(outputs[0])\n",
    "            pass\n",
    "            \n",
    "        # 3. Parse Output\n",
    "        # Expected: \"Action: v_lin=0.2, v_ang=0.1\"\n",
    "        try:\n",
    "            v_lin = float(re.search(r\"v_lin=([0-9.-]+)\", output_text).group(1))\n",
    "            v_ang = float(re.search(r\"v_ang=([0-9.-]+)\", output_text).group(1))\n",
    "        except:\n",
    "            print(f\"Failed to parse: {output_text}\")\n",
    "            v_lin, v_ang = 0.0, 0.0 # Fail-safe stop\n",
    "            \n",
    "        return v_lin, v_ang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Evaluation Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup World\n",
    "gen = HospitalGenerator(width=20, height=20)\n",
    "gen.initialize_map()\n",
    "gen.generate_layout(num_wards=1)\n",
    "world_config = gen.to_dict()\n",
    "\n",
    "runner = EpisodeRunner(world_config)\n",
    "policy = VLAPolicy(model)\n",
    "\n",
    "# Run Loop\n",
    "import math\n",
    "start = (2.0, 10.0, 0.0)\n",
    "goal = (18.0, 10.0, 0.0)\n",
    "\n",
    "x, y, theta = start\n",
    "t = 0.0\n",
    "dt = 0.1\n",
    "trajectory = []\n",
    "\n",
    "print(\"Starting VLA Inference Loop...\")\n",
    "for _ in range(50): # Short test\n",
    "    # Mock Sensor\n",
    "    d_nearest = 2.0\n",
    "    \n",
    "    state = RobotState(t, x, y, theta, 0, 0)\n",
    "    v, w = policy.get_action(state, goal, d_nearest)\n",
    "    \n",
    "    # Step\n",
    "    x += v * math.cos(theta) * dt\n",
    "    y += v * math.sin(theta) * dt\n",
    "    theta += w * dt\n",
    "    t += dt\n",
    "    trajectory.append({'x': x, 'y': y})\n",
    "    \n",
    "print(f\"Finished. Final Pose: ({x:.2f}, {y:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
