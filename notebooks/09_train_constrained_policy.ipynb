{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 09: Train Constrained Policy (Lagrangian CMDP)\n",
    "\n",
    "This notebook implements the **Constrained Policy Optimization** loop.\n",
    "For this V0.1 Benchmark, we use a simplified **Heuristic Parameter Optimization** driven by the Lagrangian.\n",
    "\n",
    "**Objective**:\n",
    "Maximize Velocity (Approximated Reward) subject to $SVR \\le 0.05$ (5%).\n",
    "\n",
    "**Method**:\n",
    "1. Run batch of episodes.\n",
    "2. Compute average Cost (SVR).\n",
    "3. Update Lagrange Multiplier $\\lambda$ based on constraint violation.\n",
    "4. Update Policy Parameters ($k_{safe}$) based on $\\lambda$.\n",
    "5. Repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "from world_gen.hospital_generator import HospitalGenerator\n",
    "from simulation.episode_runner import EpisodeRunner\n",
    "from metrics.safety_evaluator import SafetyEvaluator\n",
    "from policy.constrained_policy import ConstrainedPolicy, LagrangianOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a training world (or generate simple one)\n",
    "gen = HospitalGenerator(width=20, height=20)\n",
    "gen.initialize_map()\n",
    "gen.generate_layout(num_wards=2)\n",
    "world_config = gen.to_dict()\n",
    "\n",
    "policy = ConstrainedPolicy()\n",
    "optimizer = LagrangianOptimizer(target_cost=0.05, lr=2.0) # Target 5% SVR\n",
    "\n",
    "history = {\"cost\": [], \"lambda\": [], \"k_safe\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "EPISODES_PER_BATCH = 5\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_svr = 0.0\n",
    "    \n",
    "    # Run Batch\n",
    "    for _ in range(EPISODES_PER_BATCH):\n",
    "        # Custom runner logic needed to inject policy?\n",
    "        # The Mock EpisodeRunner inside currently does P-Control hardcoded.\n",
    "        # We need to OVERRIDE it, or modify EpisodeRunner to accept a policy.\n",
    "        # For this notebook, let's write a quick loop that uses the policy object directly.\n",
    "        \n",
    "        start = (2.0, 10.0, 0.0)\n",
    "        goal = (18.0, 10.0, 0.0)\n",
    "        \n",
    "        # Manual Episode Loop\n",
    "        traj = []\n",
    "        runner = EpisodeRunner(world_config)\n",
    "        \n",
    "        # Re-implement simple step loop here to use policy.get_action\n",
    "        # (In a real system, EpisodeRunner would take policy as arg)\n",
    "        # Quick patch:\n",
    "        from simulation.episode_runner import RobotState\n",
    "        import math\n",
    "        \n",
    "        # Init State\n",
    "        x, y, theta = start\n",
    "        t = 0.0\n",
    "        dt = 0.1\n",
    "        \n",
    "        ep_log = []\n",
    "        \n",
    "        for _ in range(300): # 30s limit\n",
    "            # 1. Distances\n",
    "            d_nearest = {\"bed\": 99.9, \"person\": 99.9}\n",
    "            # Quick brute force search\n",
    "            for obj in world_config['objects']:\n",
    "                d = math.sqrt((x - obj['pose']['x'])**2 + (y - obj['pose']['y'])**2)\n",
    "                if obj['type'] in d_nearest:\n",
    "                    d_nearest[obj['type']] = min(d_nearest[obj['type']], d)\n",
    "            \n",
    "            # 2. Policy Action\n",
    "            r_state = RobotState(t, x, y, theta, 0, 0)\n",
    "            v, w = policy.get_action(r_state, goal, d_nearest)\n",
    "            \n",
    "            # 3. Step\n",
    "            x += v * math.cos(theta) * dt\n",
    "            y += v * math.sin(theta) * dt\n",
    "            theta += w * dt\n",
    "            t += dt\n",
    "\n",
    "            ep_log.append({'t': t, 'x': x, 'y': y, 'v_lin': v, 'v_ang': w})\n",
    "            \n",
    "            # Goal Check\n",
    "            dist_goal = math.sqrt((x-goal[0])**2 + (y-goal[1])**2)\n",
    "            if dist_goal < 0.2: break\n",
    "        \n",
    "        # Evaluate\n",
    "        evaluator = SafetyEvaluator(world_config['objects'])\n",
    "        metrics, _ = evaluator.evaluate_episode(pd.DataFrame(ep_log))\n",
    "        total_svr += metrics['SVR']\n",
    "        \n",
    "    avg_svr = total_svr / EPISODES_PER_BATCH\n",
    "    \n",
    "    # Update\n",
    "    lambda_val = optimizer.update(avg_svr)\n",
    "    policy.update_params(lambda_val)\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Cost(SVR)={avg_svr:.3f}, Lambda={lambda_val:.3f}, K_safe={policy.params.k_safe_person:.3f}\")\n",
    "    \n",
    "    history['cost'].append(avg_svr)\n",
    "    history['lambda'].append(lambda_val)\n",
    "    history['k_safe'].append(policy.params.k_safe_person)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('SVR Cost', color='red')\n",
    "ax1.plot(history['cost'], color='red', label='SVR')\n",
    "ax1.tick_params(axis='y', labelcolor='red')\n",
    "ax1.axhline(y=0.05, color='gray', linestyle='--', label='Target (0.05)')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Lambda / K_safe', color='blue')\n",
    "ax2.plot(history['lambda'], color='blue', linestyle='--', label='Lambda')\n",
    "ax2.plot(history['k_safe'], color='green', label='K_safe')\n",
    "ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "plt.title(\"Lagrangian Constraint Learning\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
